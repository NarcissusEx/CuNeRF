
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>SparseNeRF</title>

    <meta name="description" content="Project page for SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis">
    <meta name="viewport" content="width=device-width, initial-scale=1">

        <!--FACEBOOK-->
    <meta property="og:image" content="img/teaser.jpg">
    <meta property="og:image:type" content="image/jpg">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://sparsenerf.github.io/"/>
    <meta property="og:title" content="SparseNeRF" />
    <meta property="og:description" content="Project page for SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis" />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="SparseNeRF" />
    <meta name="twitter:description" content="Project page for SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis" />
    <meta name="twitter:image" content="img/teaser.jpg" />

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="../../stylesheet.css">
    <!-- <link rel="stylesheet" href="css/bootstrap.min.css"> -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script type="text/javascript">
        function toggle_visibility(id) {
           var e = document.getElementById(id);
           if(e.style.display == 'block')
              e.style.display = 'none';
           else
              e.style.display = 'block';
        }
        </script>
</head>

<body>
    <script type="importmap">
        {
            "imports": {
                "three": "./js/three.module.js"
            }
        }
    </script>
    <script type="module" src='js/renderer.js'></script>
    <!-- The Modal -->
    <div id="myModal" class="modal">
        <!-- Modal content -->
        <div class="modal-content">
            <!-- <span class="close">&times;</span> -->
            <div class="row" style="align-content: center; width: 100%;">
                <div class="col-lg-8" style='padding-right:5px; padding-left:0px; '>
                    <div id="pano-container" style='text-align: center; height: 100%;'>
                        <img id="pano-img" src="" style="display: none;">
                    </div>
                </div>
                <div class="col-lg-4" style='padding-left:5px; padding-right:0px; height: 100%;'>
                    <div id="threejs-dynamic-container" style='text-align: center; height: 100%;'>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="container" id="main" style="width: 100%; max-width: 1500px;">
        <div class="row">
            <h1 class="col-md-12 text-center">
                <b>SparseNeRF</b>: Distilling Depth Ranking for Few-shot Novel View Synthesis <br>
                <small>
                    ICCV 2023
                </small>
            </h1>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://wanggcong.github.io/">
                            Guangcong Wang
                        </a>
                    </li>
                    <li>
                        <a href="https://frozenburning.github.io">
                            Zhaoxi Chen
                        </a>
                    </li>
                    <li>
                        <a href="https://www.mmlab-ntu.com/person/ccloy/index.html">
                            Chen Change Loy
                        </a>
                    </li>
                    <li>
                        <a href="http://liuziwei7.github.io">
                            Ziwei Liu
                        </a>
                    </li>
                    </br>Nanyang Technological University
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2303.16196">
                            <image src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01-p-500.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtu.be/V0yCTakA964">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/Wanggcong/SparseNeRF">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p class="text-justify">
                <strong>TL;DR:</strong> We present SparseNeRF, a simple yet effective method that synthesizes novel views given a few images. SparseNeRF distills robust local depth ranking priors from real-world inaccurate depth observations, such as pre-trained monocular depth estimation models or consumer-level depth sensors.
                </p>
                <video id="v0" width="100%" autoplay loop muted controls>
                    <source src="img/demo_video_v8_task.mp4" type="video/mp4"/>
                </video>
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Neural Radiance Field (NeRF) significantly degrades when only a limited number of views are available. To complement the lack of 3D information, depth-based models, such as DSNeRF and MonoSDF, explicitly assume the availability of accurate depth maps of multiple views. They linearly scale the accurate depth maps as supervision to guide the predicted depth of few-shot NeRFs. However, accurate depth maps are difficult and expensive to capture due to wide-range depth distances in the wild. 
                    In this work, we present a new Sparse-view NeRF (<strong>SparseNeRF</strong>) framework that exploits depth priors from real-world inaccurate observations. The coarse depth observations are either from pre-trained depth models or coarse depth maps of consumer-level depth sensors. Since coarse depth maps are not strictly scaled to the ground-truth depth maps, we propose a simple yet effective constraint, a local depth ranking method, on NeRFs such that the expected depth ranking of the NeRF is consistent with that of the coarse depth maps in local patches. To preserve the spatial continuity of the estimated depth of NeRF, we further propose a spatial continuity constraint to encourage the consistency of the expected depth continuity of NeRF with coarse depth maps. Surprisingly, with simple depth ranking constraints, SparseNeRF outperforms all state-of-the-art few-shot NeRF methods (including depth-based models) on standard LLFF and DTU datasets. Moreover, we collect a new dataset NVS-RGBD that contains real-world depth maps from Azure Kinect, ZED 2, and iPhone 13 Pro. Extensive experiments on NVS-RGBD dataset also validate the superiority and generalizability of SparseNeRF.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Motivation
                </h3>
                <br>
                <video id="v0" width="100%" autoplay loop muted controls>
                    <source src="img/demo_video_v8_motivation.mp4" type="video/mp4"/>
                </video>
                <p class="text-justify">
                    Depth maps are coarse: (a) inconsistent 3D geometry; (b) and (c) time jittering; (d) scale-invariant error. Directly scaling the coarse depth maps to a NeRF leads to inconsistent geometry against the expected depth of the NeRF. Instead of directly supervising a NeRF with coarse depth priors, we relax hard depth constraints and distill robust local depth ranking from the coarse depth maps to a NeRF such that the depth ranking of a NeRF is consistent with that of coarse depth. That is, we supervise a NeRF with relative depth instead of absolute depth.
               </p>
            </div>
        </div>
    
    
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Framework
                </h3>
                <p style="text-align:center;">
                    <image src="img/framework.png" class="img-responsive" alt="scales">
                </p>
                <p class="text-justify">
                    Framework Overview. SparseNeRF consists of two streams, i.e., NeRF and depth prior distillation. As for NeRF, we use Mip-NeRF as the backbone. we use a NeRF reconstruction loss. As for depth prior distillation, we distill depth priors from a pre-trained depth model. Specifically, we propose a local depth ranking regularization and a spatial continuity regularization to distill robust depth priors from coarse depth maps.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Demo Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/V0yCTakA964" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results: with three training views
                </h3>
                <br>
                <video id="v0" width="100%" autoplay loop muted controls>
                    <source src="img/demo_video_v8_short_new.mp4" type="video/mp4"/>
                </video>
            </div>
        </div>

    
    
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10" style="padding: 0%; margin: 0%;">
                    <textarea id="bibtex" class="form-control" readonly>
@article{wang2023sparsenerf,
    title={SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis},
    author={Guangcong and Zhaoxi Chen and Chen Change Loy and Ziwei Liu},
    journal={IEEE/CVF International Conference on Computer Vision (ICCV)},
    year={2023}}
                    </textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Related Links
                </h3>
                <p class="text-justify">
                    <a href="https://scene-dreamer.github.io/">SceneDreamer</a>: Unbounded 3D Scene Generation from 2D Image Collections. <br>
                    <a href="https://classifier-as-generator.github.io/">CaG</a>: Traditional Classification Neural Networks are Good Generators: They are Competitive with DDPMs and GANs.<br>
                    <a href="https://frozenburning.github.io/projects/text2light/">Text2light</a>: Zero-Shot Text-Driven HDR Panorama Generation. <br>
                    <a href="https://style-light.github.io/">StyleLight</a> generates HDR indoor panorama from a limited FOV image. <br>
                    <a href="https://fast-vid2vid.github.io/">Fast-Vid2Vid</a>: Spatial-Temporal Compression for Video-to-Video Synthesis. <br>
                    <a href="https://hongfz16.github.io/projects/AvatarCLIP.html">AvatarCLIP</a> proposes a zero-shot text-driven framework for 3D avatar generation and animation. <br>
                    <a href="https://yumingj.github.io/projects/Text2Human.html">Text2Human</a> proposes a text-driven controllable human image generation framework. <br>
                    <a href="https://frozenburning.github.io/projects/relighting4d/">Relighting4D</a> can relight human actors using the HDRI generated by us. <br>
                </p>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    This work is supported by the National Research Foundation, Singapore under its AI Singapore Programme, NTU NAP, MOE AcRF Tier 2 (T2EP20221-0033), and under the RIE2020 Industry Alignment Fund - Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).
                    <br>


                The website template is borrowed from <a href="https://jonbarron.info/mipnerf/">Mip-NeRF</a>.
                </p>
                <br>
            </div>
        </div>

         

    
    <!-- this is comment-->
            

    <!-- <div class="section" style="width:200px; margin-left: 40%;">  -->
    <div class="section" style="text-align:center; padding:0 0 20px 0">
        <a href='https://clustrmaps.com/site/1bttf'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=tt&d=ON2inUjCQJIOYgEVeUFiZy0DGmbC8DbWuTjJ5QVVhII'/></a>
    </div>


            
  </div> <!-- #content -->           
                    
  </div>
</body>

</html>
